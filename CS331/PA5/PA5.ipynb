{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb8b41d",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **PA5:  Unsupervised Learning**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a974587",
   "metadata": {},
   "source": [
    "\n",
    "# **Part¬†1¬†:Song Clustering with K‚ÄëMeans(80¬†marks)**\n",
    "\n",
    "In this part you will perform unsupervised clustering of\n",
    "Spotify track features using the classic **K‚ÄëMeans** algorithm.\n",
    "\n",
    "You will:\n",
    "\n",
    "1. Load and inspect the Spotify¬†Features dataset.  \n",
    "2. Drop purely¬†metadata columns that should not influence clustering.  \n",
    "3. Standardise numeric features (K‚ÄëMeans is distance‚Äëbased).  \n",
    "4. Use the **Elbow** and **Silhouette** methods to choose an appropriate¬†*K*.  \n",
    "5. Train a final K‚ÄëMeans model and attach cluster labels.  \n",
    "6. Visualise clusters in 2‚ÄëD via **PCA**.  \n",
    "\n",
    "You are <span style=\"color: red;\">not allowed</span> to use any libraries that have not been imported already\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707de89",
   "metadata": {},
   "source": [
    "# **Step¬†1¬†: Import¬†Libraries**\n",
    "\n",
    "All of the necessary libraries for this part have been imported for you below. You may not use any other library apart from standard Python librares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a08374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are not allowed import any additional libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478a292",
   "metadata": {},
   "source": [
    "# **Step 2 : Load & Clean the Dataset** (5 marks)\n",
    "\n",
    "Load the `SpotifyFeatures.csv` dataset into a DataFrame.\n",
    "\n",
    "In this step, your goal is to:\n",
    "- Load the dataset\n",
    "- Check for any **null values**\n",
    "- Fill or drop null values as appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19e272",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 2: Load the dataset over here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de893882",
   "metadata": {},
   "source": [
    "# Feature Selection Debate ‚Äî What Should We Do with Categorical Columns? (5 marks)\n",
    "\n",
    "We asked three of your TAs ‚Äî **Abdullah**, **Ahsan**, and **Hamza** ‚Äî how to handle **categorical features** in the Spotify dataset, such as:\n",
    "\n",
    "- `genre` (e.g., \"Pop\", \"Jazz\", \"Hip-Hop\")  \n",
    "- `key` (e.g., \"C#\", \"D\", \"G\")  \n",
    "- `time_signature` (e.g., \"3/4\", \"4/4\")  \n",
    "- `mode` (e.g., \"Major\", \"Minor\")\n",
    "\n",
    "Here‚Äôs what each TA had to say:\n",
    "\n",
    "---\n",
    "\n",
    "### TA Opinions\n",
    "\n",
    "- **Abdullah** says:  \n",
    "  ‚ÄúWe should remove all categorical columns. K-Means relies on numerical distance, using categories will distort the clustering.‚Äù\n",
    "\n",
    "- **Ahsan** says:  \n",
    "  ‚ÄúWe should keep useful categorical features and label encode them.  \n",
    "  For example: `Pop = 0`, `Jazz = 1`, `Hip-Hop = 2`. It‚Äôs compact and doesn‚Äôt add extra columns.‚Äù\n",
    "\n",
    "- **Hamza** says:  \n",
    "  ‚ÄúWe should apply **one-hot encoding**, but only to **carefully chosen** categorical features:  \n",
    "  - Keep those that are meaningful and have few categories (like `time_signature`)  \n",
    "  - Avoid encoding features with too many unique values (like `genre`)  \n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "- Whose approach do you agree with, and why?  \n",
    "- Think critically about how categorical features might affect clustering results.\n",
    "\n",
    "**Write your answer in the markdown cell below.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207517aa",
   "metadata": {},
   "source": [
    "## Answer Here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709d8d8",
   "metadata": {},
   "source": [
    "# **Step 3: Drop All Categorical Features**\n",
    "\n",
    "To keep this assignment focused and manageable, we‚Äôll **remove all categorical features** for now.\n",
    "\n",
    "That means you don‚Äôt need to worry about encoding or choosing which ones to keep. We‚Äôre doing this to simplify the clustering process and let you concentrate on the core idea behind K-Means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037250d",
   "metadata": {},
   "source": [
    "# **Step 4 : Standardize the Numerical Features** (10 marks)\n",
    "\n",
    "Before standardizing the features, take a moment to think:\n",
    "\n",
    "- Why do we actually need to standardize the features before applying K-Means clustering?  \n",
    "- Don‚Äôt give a generic answer, instead, think about what the K-Means algorithm is actually doing when it forms clusters.\n",
    "\n",
    "An example of a **generic answer to avoid**:\n",
    "- We standardize so that all the features are on the same scale.‚Äù\n",
    "\n",
    "That's not enough. Think deeper. What does the algorithm actually do? Why might raw numerical values affect that process?\n",
    "\n",
    "\n",
    "üìù **Your Task**:  \n",
    "Write your reasoning under the \"Your Reasoning Heading\" below.\n",
    "\n",
    "## **Your Reasoning**:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Finally, you may proceed to **standardize the numerical features** in the code cell.\n",
    "\n",
    "## **Standardization Instructions**\n",
    "\n",
    "You must manually compute **z-scores** for each numeric feature without using `StandardScaler` or any machine learning library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50638bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c1aa3c",
   "metadata": {},
   "source": [
    "# **Step 4: Implement K-Means as a Standalone Function** (20 marks)\n",
    "\n",
    "Before you can evaluate different values of K or analyze cluster structure, you need to implement **K-Means** yourself ‚Äî without using any ML libraries.\n",
    "\n",
    "You will write a **standalone, reusable function** that can be used for:\n",
    "- Computing WCSS for the Elbow method\n",
    "- Getting cluster labels for the Silhouette score\n",
    "- Running the final clustering model\n",
    "\n",
    "---\n",
    "\n",
    "## What Should Your K-Means Function Do?\n",
    "\n",
    "Your function will take in:\n",
    "- A 2D NumPy array `X` of standardized feature data\n",
    "- A number of clusters `k`\n",
    "- Optional parameters like number of iterations and convergence tolerance\n",
    "\n",
    "It will output:\n",
    "- Final cluster **centroids**\n",
    "- Cluster **labels** (which cluster each point belongs to)\n",
    "- The total **WCSS** (Within-Cluster Sum of Squares)\n",
    "\n",
    "This will allow you to reuse the same logic for all the later steps of the assignment.\n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs the Core Idea of K-Means?\n",
    "\n",
    "Here‚Äôs the step-by-step logic:\n",
    "\n",
    "1. **Initialize**:  \n",
    "   - Randomly choose `k` points from the dataset to be the **initial centroids**.\n",
    "\n",
    "2. **Assign Points to Clusters**:  \n",
    "   - For each data point, compute its distance to each centroid.\n",
    "   - Assign it to the nearest one.\n",
    "\n",
    "3. **Update Centroids**:  \n",
    "   - For each cluster, compute the **mean** of all points assigned to it.\n",
    "   - This becomes the **new centroid**.\n",
    "\n",
    "4. **Repeat**:  \n",
    "   - Continue reassigning points and updating centroids until the centroids stop changing significantly, or a maximum number of iterations is reached.\n",
    "\n",
    "---\n",
    "\n",
    "## How Will You Use This Later?\n",
    "\n",
    "Once you‚Äôve implemented this function, you‚Äôll call it repeatedly in future steps:\n",
    "- To compute WCSS for multiple K values (for the Elbow method)\n",
    "- To get cluster labels for computing Silhouette Scores\n",
    "- To get the final clustering result when you pick your best K\n",
    "\n",
    "This avoids writing the same logic over and over again.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, k, max_iters=100, tol=1e-4, verbose=False):\n",
    "    \"\"\"\n",
    "    K-Means clustering from scratch.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray (n_samples, n_features), standardized data\n",
    "    - k: number of clusters\n",
    "    - max_iters: max number of iterations\n",
    "    - tol: tolerance for centroid movement (L2 norm)\n",
    "    - verbose: print internal info if True\n",
    "\n",
    "    Returns:\n",
    "    - centroids: final (k, n_features) array of cluster centers\n",
    "    - labels: (n_samples,) array of assigned cluster indices\n",
    "    - wcss: total within-cluster sum of squares\n",
    "    \"\"\"\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc2e92",
   "metadata": {},
   "source": [
    "# **Step 5: Choose‚ÄØK with the Elbow (WCSS) Method** (15 marks)\n",
    "\n",
    "Before we can run K-Means, we have to choose how many clusters (**K**) we want.  \n",
    "But how do we know what a ‚Äúgood‚Äù K is?\n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs WCSS?\n",
    "\n",
    "WCSS stands for **Within-Cluster Sum of Squares**. It‚Äôs a number that tells us **how tight and compact our clusters are**.\n",
    "\n",
    "Here‚Äôs the idea:\n",
    "\n",
    "- Each point belongs to a cluster.\n",
    "- That cluster has a center (called a **centroid**).\n",
    "- For each point, we calculate how far it is from the center of its cluster.\n",
    "- We square those distances and **add them all up** across all clusters.\n",
    "\n",
    "This total is the WCSS.  \n",
    "If your clusters are tight (all points close to the center), WCSS is low.  \n",
    "If your clusters are messy and spread out, WCSS is high.\n",
    "\n",
    "---\n",
    "\n",
    "## What Is the Elbow Method?\n",
    "\n",
    "Let‚Äôs say we try different values of K, like 2, 3, 4, all the way to 10 ‚Äî and calculate the WCSS each time.\n",
    "\n",
    "We then plot:\n",
    "- X-axis = number of clusters (K)\n",
    "- Y-axis = WCSS\n",
    "\n",
    "Here‚Äôs what usually happens:\n",
    "- At first, WCSS drops **a lot** as you increase K. That‚Äôs good ‚Äî your clusters are getting tighter.\n",
    "- But after a point, adding more clusters doesn‚Äôt help much ‚Äî WCSS still drops, but only **a little**.\n",
    "\n",
    "That turning point, where the drop starts to slow down, is called the **elbow**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Pick the Elbow?\n",
    "\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "- When you first increase K (e.g., from 2 to 3 to 4), K-Means is:\n",
    "  - It‚Äôs discovering **natural groupings** in your data ‚Äî separating points that belong in different clusters.\n",
    "  - That‚Äôs why the WCSS (total error) drops **a lot** in the beginning ‚Äî the clusters are becoming more meaningful and distinct.\n",
    "\n",
    "But as you keep increasing K‚Ä¶\n",
    "\n",
    "- At some point, the algorithm has already found the **main clusters** in the data.\n",
    "- Any further increase in K just means:\n",
    "  - You're splitting existing clusters into smaller sub-groups.\n",
    "  - These new clusters don‚Äôt reveal new structure ‚Äî they just divide things **within** already good clusters.\n",
    "  - So the improvement in WCSS becomes **very small**.\n",
    "\n",
    "That point ‚Äî where we go from **discovering real structure** to just **splitting things for no good reason** ‚Äî is what we call the **elbow**.\n",
    "\n",
    "It‚Äôs where the WCSS curve changes from steep to flat.\n",
    "\n",
    "So we pick the elbow as our K because:\n",
    "- Before the elbow: we‚Äôre **finding real, meaningful clusters**.\n",
    "- After the elbow: we‚Äôre just **overcomplicating** the model by breaking up groups that were already good.\n",
    "\n",
    "---\n",
    "## What About Random Initialization?\n",
    "\n",
    "One small problem with K-Means:  \n",
    "It starts with **random initial centroids** ‚Äî and if it gets unlucky, it can give a **bad result**.\n",
    "\n",
    "That‚Äôs why we don‚Äôt just run it once.\n",
    "\n",
    "Instead, we run it **multiple times** (e.g., 5 random initializations), and keep the clustering with the **lowest WCSS**.\n",
    "\n",
    "This makes the elbow curve more **reliable**, because:\n",
    "- It avoids misleading results from a single unlucky run.\n",
    "- It gives you the best possible WCSS value for each K.\n",
    "\n",
    "So when you see a WCSS point on the elbow plot, know that it‚Äôs **not from one run** ‚Äî it‚Äôs the best result from multiple trials with different random starting points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd29325",
   "metadata": {},
   "source": [
    "## What You Need to Do\n",
    "\n",
    "Now that you‚Äôve implemented your `kmeans()` function, you will use it to apply the **Elbow Method** for identifying a suitable value of **K**.\n",
    "\n",
    "Follow the steps below:\n",
    "\n",
    "---\n",
    "\n",
    "1. Run K-Means for different values of **K** ranging from **2 to 10**.\n",
    "\n",
    "2. For each K, calculate the **WCSS** (Within-Cluster Sum of Squares) ‚Äî this value is already returned by your `kmeans()` function.\n",
    "\n",
    "3. **Important:**  \n",
    "   Since K-Means starts with **random initial centroids**, it can produce different results on each run.  \n",
    "   To get a more **reliable WCSS**, you must:\n",
    "   - Run K-Means **5 times for each K**\n",
    "   - Store the **lowest WCSS** among those 5 runs\n",
    "\n",
    "   This avoids misleading results due to a single unlucky initialization.\n",
    "\n",
    "4. Store the final (lowest) WCSS value for each K, and plot a line graph:\n",
    "   - **X-axis** = values of K (2 through 10)  \n",
    "   - **Y-axis** = corresponding WCSS values\n",
    "\n",
    "5. **Visually inspect the plot** and look for the **\"elbow\"** ‚Äî the point where WCSS stops decreasing rapidly.  \n",
    "   This is the K where adding more clusters doesn‚Äôt significantly improve the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a447f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_wcss_range(X, k_range, n_init=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Computes WCSS for a range of cluster counts using the lowest WCSS out of multiple initializations.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray of shape (n_samples, n_features)\n",
    "    - k_range: iterable of K values to evaluate (e.g., range(2, 11))\n",
    "    - n_init: number of times to run KMeans per K\n",
    "    - verbose: whether to print status\n",
    "\n",
    "    Returns:\n",
    "    - wcss_values: list of lowest WCSS per K\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "# Plot the curve here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad188cf",
   "metadata": {},
   "source": [
    "# **Step 6: Validate K with the Silhouette Score** (15 marks)\n",
    "\n",
    "Once you‚Äôve used the Elbow Method to narrow down a possible K, it‚Äôs helpful to **validate** your choice using another metric: the **Silhouette Score**.\n",
    "\n",
    "---\n",
    "\n",
    "## What‚Äôs the Silhouette Score?\n",
    "\n",
    "The **Silhouette Score** measures how well each point fits within its assigned cluster **compared to** other clusters.\n",
    "\n",
    "It answers the question:\n",
    "> ‚ÄúIs this point really in the right cluster, or would it fit better somewhere else?‚Äù\n",
    "\n",
    "Here‚Äôs how it works:\n",
    "\n",
    "For each data point:\n",
    "- `a(i)` = average distance to other points in the **same cluster**\n",
    "- `b(i)` = average distance to points in the **nearest other cluster**\n",
    "- Silhouette score for that point =  \n",
    "  **(b - a) / max(a, b)**\n",
    "\n",
    "So:\n",
    "- A score near **+1** ‚Üí well-clustered\n",
    "- A score near **0** ‚Üí ambiguous or on a boundary\n",
    "- A score **< 0** ‚Üí possibly misclassified\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use the Silhouette Score?\n",
    "\n",
    "Unlike WCSS (which only measures **how compact** your clusters are), the Silhouette Score measures **how well-separated** and **internally consistent** they are.\n",
    "\n",
    "It captures two ideas:\n",
    "- **Cohesion**: Are the points close to others in their own cluster?\n",
    "- **Separation**: Are they far from other clusters?\n",
    "\n",
    "This makes it ideal for validating your K.\n",
    "\n",
    "---\n",
    "\n",
    "## What You Need to Do\n",
    "\n",
    "1. Run your `kmeans()` function for each value of **K** from 2 to 10.\n",
    "2. For each K, run K-Means **5 times** with different random initializations.\n",
    "3. For each run, compute the **average silhouette score** using `compute_average_silhouette()`.\n",
    "4. For each K, keep the **highest silhouette score** among the 5 runs.\n",
    "5. Use `evaluate_silhouette_over_k()` to automate this process.\n",
    "6. Plot the results using `plot_silhouette_scores()`.\n",
    "\n",
    "---\n",
    "\n",
    "## A little explanation of the functions\n",
    "\n",
    "### `compute_average_silhouette(X, labels)`\n",
    "- Computes the silhouette score **from scratch**.\n",
    "- Input: dataset `X`, assigned cluster labels `labels`\n",
    "- Output: average silhouette score (float)\n",
    "\n",
    "### `evaluate_silhouette_over_k(X, k_range, n_init=5, sample_size=1000)`\n",
    "- Tries multiple K values and multiple initializations.\n",
    "- Uses `compute_average_silhouette()` internally.\n",
    "- Returns a dictionary mapping each K to its best silhouette score.\n",
    "\n",
    "### `plot_silhouette_scores(scores)`\n",
    "- Takes the output of the previous function and draws a line plot.\n",
    "- Helps you visually identify the **best K** (the one with the peak score).\n",
    "\n",
    "---\n",
    "\n",
    "## What You‚Äôre Looking For\n",
    "\n",
    "- A clear **peak** in silhouette score across different values of K.\n",
    "- The best K is typically the one with the **highest average silhouette score** ‚Äî not too small (underfitting) and not too large (overfitting).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a652d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_silhouette(X, labels):\n",
    "    \"\"\"\n",
    "    Computes the average silhouette score from scratch.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray of shape (n_samples, n_features)\n",
    "    - labels: ndarray of shape (n_samples,) with cluster labels\n",
    "\n",
    "    Returns:\n",
    "    - mean silhouette score (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "def evaluate_silhouette_over_k(X, k_range, n_init=5, sample_size=1000, random_state=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluates silhouette scores for different values of K using custom KMeans and silhouette.\n",
    "\n",
    "    Parameters:\n",
    "    - X: ndarray (n_samples, n_features)\n",
    "    - k_range: iterable of K values to evaluate\n",
    "    - n_init: number of KMeans initializations per K\n",
    "    - sample_size: max number of points to use for silhouette score\n",
    "    - random_state: RNG seed\n",
    "    - verbose: print progress\n",
    "\n",
    "    Returns:\n",
    "    - silhouette_scores: dict {K: best silhouette score}\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_silhouette_scores(scores):\n",
    "    \"\"\"\n",
    "    Plots silhouette scores over different values of K.\n",
    "\n",
    "    Parameters:\n",
    "    - scores: dict {K: silhouette score}\n",
    "    \"\"\"\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a2a2a",
   "metadata": {},
   "source": [
    "# **Step¬†7¬†: Train¬†Final¬†K‚ÄëMeans¬†Model** (5¬†marks)\n",
    "\n",
    "Choose the best K based on the elbow and silhouette plots.  \n",
    "Then, train a final K-Means model using that K.\n",
    "We fit the final model and attach the resulting cluster labels to¬†`df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "best_k = 999  # Replace with the value you selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4127c58",
   "metadata": {},
   "source": [
    "# **Step 8: Visualize Clusters via PCA** (10 marks)\n",
    "\n",
    "After training your final K-Means model, it‚Äôs important to **see what the clusters look like**.\n",
    "\n",
    "But there‚Äôs a challenge: your dataset might have **many features** (dimensions), which humans can‚Äôt directly visualize.\n",
    "\n",
    "---\n",
    "\n",
    "## Why PCA?\n",
    "\n",
    "We use **Principal Component Analysis (PCA)** to solve this.\n",
    "\n",
    "PCA is a dimensionality reduction technique that transforms your data into a new coordinate system where:\n",
    "- The **first axis (PC1)** captures the most variation in the data\n",
    "- The **second axis (PC2)** captures the next most variation (orthogonal to the first)\n",
    "\n",
    "This lets us compress the data into just **2 dimensions**, while keeping as much structure as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Visualize Clusters?\n",
    "\n",
    "Once we project the data using PCA, we can:\n",
    "- Plot each data point in this 2D space\n",
    "- Color the points based on their **cluster labels**\n",
    "\n",
    "This helps you:\n",
    "- See how well-separated the clusters are\n",
    "- Spot overlaps or outliers\n",
    "- Get an intuitive sense of whether the clusters **make sense visually**\n",
    "\n",
    "Even though PCA simplifies the data, **it often reveals structure** that's useful for understanding your clustering results.\n",
    "\n",
    "---\n",
    "\n",
    "## What You Need to Do\n",
    "\n",
    "1. Use `PCA(n_components=2)` from `sklearn.decomposition` on your **standardized dataset**.\n",
    "2. Transform the data and plot it in 2D using `matplotlib` or `seaborn`.\n",
    "3. Use different colors for each cluster using the labels from your final K-Means model.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ac7e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# You can use the PCA class from sklearn.decomposition to reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33202c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that you've visualized your clusters in 2D using PCA, take a moment to **analyze what you're seeing**:\n",
    "\n",
    "- Are the clusters **visually separated**, or do they **overlap**?\n",
    "- Does the shape and size of each cluster **look balanced**, or is one dominating the space?\n",
    "- Can you spot any **outliers** or points that look like they could belong to more than one cluster?\n",
    "\n",
    "> ‚úçÔ∏è **Your Task**:  \n",
    "Write a short explanation (3‚Äì4 lines) interpreting the PCA plot above. Focus on:\n",
    "- Whether the clusters are well-separated\n",
    "- Whether this visualization supports your earlier decision for **K = 3**\n",
    "\n",
    "Remember: PCA reduces dimensionality, so this is an approximation ‚Äî but it‚Äôs still a **great sanity check** to confirm your clustering results.\n",
    "\n",
    "\n",
    " **Your Answer**\n",
    "-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd298229",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "üéâ **Well¬†done!**  \n",
    "You have successfully completed PA5 Part¬†1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3a745",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<h1><b> CS331: Denoising Autoencoders and Latent Space Visualization | <span style=\"color: #007BFF;\"> Assignment 5 Section 2 [120 marks] </span></b></h1>**\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a628ba4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from torchsummary import summary\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') # Apple Silicon GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf59dcd",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**<h1><b><span style=\"color: #007BFF;\"> Visualization Functions</span></b></h1>**\n",
    "\n",
    "<hr>\n",
    "\n",
    "Only use the following helper visualization functions  provided below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43898459",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =========== Visualization Functions ==========\n",
    "\n",
    "def displayImg(img, title=\"\"):\n",
    "    \"\"\"\n",
    "    Displays a tensor as an image. Handles both single images and grids of images.\n",
    "\n",
    "    Args:\n",
    "        img (torch.Tensor): The image tensor to display. Expected shape for a single image\n",
    "                           is (C, H, W) or for a grid (C, H, W_total) created by\n",
    "                           torchvision.utils.make_grid.\n",
    "        title (str, optional): The title to display above the image. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the image using matplotlib.\n",
    "    \"\"\"\n",
    "    img = img.cpu()\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_comparison(original, noisy, reconstructed, n=5, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Displays a comparison grid of original, noisy, and reconstructed images.\n",
    "\n",
    "    Args:\n",
    "        original (torch.Tensor): A batch of original (clean) image tensors.\n",
    "                                 Shape: (batch_size, C, H, W).\n",
    "        noisy (torch.Tensor): A batch of corresponding noisy image tensors.\n",
    "                              Shape: (batch_size, C, H, W).\n",
    "        reconstructed (torch.Tensor): A batch of corresponding reconstructed image tensors\n",
    "                                      output by the autoencoder. Shape: (batch_size, C, H, W).\n",
    "        n (int, optional): The number of image triplets (original, noisy, reconstructed)\n",
    "                           to display. Defaults to 5.\n",
    "        title_prefix (str, optional): A string to prepend to the main title of the plot.\n",
    "                                     Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the comparison plot using matplotlib.\n",
    "    \"\"\"\n",
    "    original = original.cpu()\n",
    "    noisy = noisy.cpu()\n",
    "    reconstructed = reconstructed.cpu()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(n):\n",
    "        # --- Display original image ---\n",
    "        ax = plt.subplot(3, n, i + 1)\n",
    "        plt.imshow(original[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(\"Original\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # --- Display noisy input image ---\n",
    "        ax = plt.subplot(3, n, i + 1 + n)\n",
    "        plt.imshow(noisy[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(\"Noisy Input\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # --- Display reconstructed image ---\n",
    "        ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "        plt.imshow(reconstructed[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(\"Reconstructed\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.suptitle(f\"{title_prefix} - Image Denoising Comparison\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440c4ee",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1><b> Introduction to Denoising Autoencoders <span style=\"color: #007BFF;\"></span></b></h1>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### <h2><b> The Challenge of Noise in Data </b></h2>\n",
    "<p>Imagine taking a photo in low light. The resulting image might appear grainy or have artifacts ‚Äì this is noise. Noise is common in real-world data and can significantly degrade its quality. For machine learning models, noisy data can lead to poor performance in tasks like image classification, object detection, or medical image analysis, as the noise can obscure the underlying patterns the model needs to learn.</p>\n",
    "\n",
    "### <h2><b> Autoencoders: Learning to Reconstruct </b></h2>\n",
    "<p>An Autoencoder is a type of artificial neural network used for unsupervised learning, primarily aimed at learning efficient representations (encodings) of data. It consists of two parts:</p>\n",
    "<ol>\n",
    "    <li><b>Encoder:</b>     Compresses the input data into a lower-dimensional latent space (also called the bottleneck). This forces the encoder to capture the most salient features of the data.</li>\n",
    "    <li><b>Decoder:</b>     Attempts to reconstruct the original input data from the compressed latent representation generated by the encoder.</li>\n",
    "</ol>\n",
    "<p>The autoencoder is trained by minimizing the difference between the original input and the reconstructed output (reconstruction error). Essentially, it tries to learn an identity function, but the bottleneck forces it to learn a useful, compressed representation along the way.</p>\n",
    "\n",
    "### <h2><b> Denoising Autoencoders (DAEs): Learning Robust Representations </b></h2>\n",
    "<p>A Denoising Autoencoder (DAE) is a specific type of autoencoder designed to tackle the problem of noise. Instead of learning to reconstruct the original input from itself, a DAE is trained to reconstruct the *original, clean* input from a *corrupted (noisy)* version of it.</p>\n",
    "<ul>\n",
    "    <li><b>Training Process:</b>\n",
    "        <ol>\n",
    "            <li>Take a clean data sample.</li>\n",
    "            <li>Artificially add noise to create a corrupted version.</li>\n",
    "            <li>Feed the <i>noisy</i> version into the DAE's encoder.</li>\n",
    "            <li>The decoder generates a reconstruction.</li>\n",
    "            <li>Calculate the reconstruction error between the decoder's output and the <i>original clean</i> data sample.</li>\n",
    "            <li>Update the DAE's weights to minimize this error.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><b>Why it Works:</b> By forcing the network to recover the underlying clean structure from noisy input, the DAE learns features that are robust to noise and capture the essential characteristics of the data distribution. It learns to separate the signal (the digit) from the noise.</li>\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2><b> Linear vs. Convolutional Denoising Autoencoders </b></h2>\n",
    "<p>In this assignment, we will implement and compare two architectures for DAEs:</p>\n",
    "<ul>\n",
    "    <li><b>Linear DAE:</b>  This architecture uses only fully connected (dense or `Linear`) layers. To process images, the input image is typically flattened into a long vector.\n",
    "    <li><b>Convolutional DAE (CDAE):</b>    This architecture leverages convolutional layers (`Conv2d`) in the encoder and corresponding transposed convolutional layers (`ConvTranspose2d`, sometimes called deconvolutional layers) in the decoder.\n",
    "</ul>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<h2><b> MNIST Dataset for Denoising </b></h2>\n",
    "<p>We will apply these DAEs to the <b>MNIST dataset</b>. MNIST is a standard benchmark dataset consisting of 70,000 images (60k training, 10k testing) of handwritten digits (0 through 9). Each image is a 28x28 pixel grayscale image. We will create noisy versions of these images by adding Gaussian noise, which will serve as the input to our DAEs, while the original clean images will be the target output for training.</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554b68e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1><b><span style=\"color: #007BFF;\"> Hyperparameters</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "<p>Hyperparameters are configuration settings used to control the learning process. They are set before training begins. Tuning these can significantly impact model performance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ecf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 15   #dont change this\n",
    "LEARNING_RATE = 0.001\n",
    "NOISE_FACTOR = 0.5  #dont change this\n",
    "LATENT_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacde78b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1><b> Dataset Loading |<span style=\"color: #007BFF;\"> MNIST (Clean & Noisy)</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "<p>For training a Denoising Autoencoder, we require two parallel datasets:</p>\n",
    "<ol>\n",
    "    <li><b>Clean Dataset:</b> The original, noise-free MNIST images. These will serve as the <i>target</i> for our autoencoder's reconstruction during loss calculation.</li>\n",
    "    <li><b>Noisy Dataset:</b> The MNIST images corrupted with artificial noise. These will serve as the <i>input</i> to our autoencoder during training and evaluation.</li>\n",
    "</ol>\n",
    "<p>We achieve this by defining two different transformation pipelines using `torchvision.transforms`.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93371ec1",
   "metadata": {},
   "source": [
    "NOTE: Pre-written code is provided for you.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Data Transformations ==================\n",
    "transform_clean = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# --- Custom Transformation to add Gaussian Noise ---\n",
    "class AddGaussianNoise(object):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to a tensor image.\n",
    "\n",
    "    Args:\n",
    "        mean (float): Mean of the Gaussian distribution used for noise. Default: 0.0.\n",
    "        std (float): Standard deviation of the Gaussian distribution used for noise. Default: 1.0.\n",
    "        noise_factor (float): A multiplier applied to the standard deviation to control the\n",
    "                              intensity of the added noise. Default: 0.5.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., std=1., noise_factor=0.5):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Applies the noise transformation.\n",
    "\n",
    "        Args:\n",
    "            tensor (torch.Tensor): Input image tensor (expected range [0.0, 1.0]).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Image tensor with added Gaussian noise, clipped to [0.0, 1.0].\n",
    "        \"\"\"\n",
    "        noise = torch.randn(tensor.size()) * self.std * self.noise_factor + self.mean\n",
    "        noisy_tensor = tensor + noise\n",
    "        return torch.clamp(noisy_tensor, 0., 1.)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"String representation of the transformation.\"\"\"\n",
    "        return self.__class__.__name__ + f'(mean={self.mean}, std={self.std}, noise_factor={self.noise_factor})'\n",
    "\n",
    "transform_noisy = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(mean=0., std=1., noise_factor=NOISE_FACTOR)\n",
    "])\n",
    "\n",
    "\n",
    "# ================ Load Clean Data ======================\n",
    "\n",
    "trainset_clean = torchvision.datasets.MNIST(root='./data',train=True,download=True,transform=transform_clean)\n",
    "# DataLoader wraps the dataset, providing batching, shuffling, etc.\n",
    "# shuffle=False is important here to ensure clean batches align with noisy batches.\n",
    "trainloader_clean = torch.utils.data.DataLoader(trainset_clean,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "testset_clean = torchvision.datasets.MNIST( root='./data',train=False,download=True,transform=transform_clean)\n",
    "testloader_clean = torch.utils.data.DataLoader(testset_clean,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "# ================ Load Noisy Data =====================\n",
    "\n",
    "trainset_noisy = torchvision.datasets.MNIST(root='./data',train=True,download=True, transform=transform_noisy)\n",
    "trainloader_noisy = torch.utils.data.DataLoader(trainset_noisy,batch_size=BATCH_SIZE,shuffle=False)\n",
    "\n",
    "testset_noisy = torchvision.datasets.MNIST(root='./data',train=False,download=True,transform=transform_noisy)\n",
    "testloader_noisy = torch.utils.data.DataLoader(testset_noisy,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8e44f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1><b> Data Visualisation |<span style=\"color: #007BFF;\"> Clean vs. Noisy MNIST</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "<p>Before building the models, let's visualize some corresponding samples from the clean and noisy datasets. This helps confirm that the noise transformation is working as expected and gives an idea of the challenge the Denoising Autoencoder will face.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c67efee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# --- Display the images ---\n",
    "# Use displayImg to visualize the first 16 images from each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af25472",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1 style=\"text-align: left;\"><b>Task 1: Linear Denoising Autoencoder | <span style=\"color: #007BFF;\">MNIST</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "<p>Our first model is a Denoising Autoencoder constructed using only <strong>Linear (fully connected) layers</strong>. This architecture requires flattening the 2D input image into a 1D vector before feeding it into the encoder. The decoder then reconstructs the flattened vector, which represents the denoised image.\n",
    "\n",
    "<strong>üö® Important: When implementing the model, ensure that both the encoder and the decoder sections utilize a maximum of 3 `nn.Linear` layers each.üö®</strong></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3601ed",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Model Definition | <span style=\"color: #007BFF;\">Linear DAE</span></h2>\n",
    "<p>We define the `LinearDenoisingAutoencoder` class. It contains an `encoder` sequence (Linear layers reducing dimensionality) and a `decoder` sequence (Linear layers increasing dimensionality back to the original). ReLU activations are used in hidden layers, and a Sigmoid activation is crucial for the final decoder layer to ensure the output pixel values are in the [0, 1] range.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b5498",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class LinearDenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the Linear Denoising Autoencoder.\n",
    "        Defines the encoder and decoder sequential blocks.\n",
    "        \"\"\"\n",
    "        super(LinearDenoisingAutoencoder, self).__init__()\n",
    "        self.encoder = ...\n",
    "        self.decoder = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the autoencoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing a batch of images.\n",
    "                              Expected shape: (batch_size, 1, 28, 28).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor representing the reconstructed flattened images.\n",
    "                          Shape: (batch_size, 784).\n",
    "        \"\"\"\n",
    "\n",
    "        reconstruction = ...\n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e408ae2",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Trainer Class | <span style=\"color: #007BFF;\">Linear DAE</span></h2>\n",
    "<p>This `LinearDAETrainer` class encapsulates the logic for training, evaluating, and visualizing the `LinearDenoisingAutoencoder`. It handles:</p>\n",
    "<ul>\n",
    "    <li>Initialization of the model, optimizer, and loss function.</li>\n",
    "    <li>The training loop (`train_epoch`), which processes batches of noisy input and calculates loss against clean targets.</li>\n",
    "    <li>The evaluation loop (`evaluate`), which measures the reconstruction MSE on the test set without updating model weights.</li>\n",
    "    <li>A main `train` method orchestrating the epochs and printing progress.</li>\n",
    "    <li>Utility methods for plotting loss curves (`plot_loss`) and visualizing the denoising results (`visualize_denoising`).</li>\n",
    "</ul>\n",
    "<p>Crucially, the loss is always computed by comparing the model's output (reconstruction from noisy input) with the corresponding original clean image.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6711c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LinearDAETrainer:\n",
    "    def __init__(self, model, learning_rate=0.001, device=None):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The LinearDenoisingAutoencoder model instance to train.\n",
    "            learning_rate (float, optional): The learning rate for the Adam optimizer.\n",
    "                                             Defaults to 0.001.\n",
    "            device (torch.device, optional): The device (e.g., 'cuda', 'mps', 'cpu') to\n",
    "                                             run the training on.\n",
    "        \"\"\"\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        # Print a summary of the model architecture\n",
    "        summary(self.model, input_size=(1, 28, 28), device=str(self.device))\n",
    "  \n",
    "\n",
    "\n",
    "    def train_epoch(self, noisy_loader, clean_loader):\n",
    "        \"\"\"\n",
    "        Performs a single epoch of training.\n",
    "\n",
    "        Iterates through the dataset, performs forward and backward passes,\n",
    "        and updates model weights. Calculates loss against clean targets.\n",
    "\n",
    "        Args:\n",
    "            noisy_loader (DataLoader): DataLoader providing batches of noisy images (input).\n",
    "            clean_loader (DataLoader): DataLoader providing corresponding batches of clean\n",
    "                                       images (target).\n",
    "\n",
    "        Returns:\n",
    "            float: The average training loss for this epoch.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def evaluate(self, noisy_loader, clean_loader):\n",
    "        \"\"\"\n",
    "        Evaluates the model's performance on the test dataset.\n",
    "\n",
    "        Calculates the reconstruction loss  between the model's output\n",
    "        (from noisy input) and the clean target images without updating weights.\n",
    "\n",
    "        Args:\n",
    "            noisy_loader (DataLoader): DataLoader for the noisy test images (input).\n",
    "            clean_loader (DataLoader): DataLoader for the clean test images (target).\n",
    "\n",
    "        Returns:\n",
    "            float: The average test loss for this epoch.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def train(self, train_noisy_loader, train_clean_loader, test_noisy_loader, test_clean_loader, num_epochs):\n",
    "        \"\"\"\n",
    "        Runs the complete training loop for a specified number of epochs.\n",
    "\n",
    "        Calls train_epoch and evaluate for each epoch and prints the progress.\n",
    "\n",
    "        Args:\n",
    "            train_noisy_loader (DataLoader): DataLoader for noisy training data.\n",
    "            train_clean_loader (DataLoader): DataLoader for clean training data.\n",
    "            test_noisy_loader (DataLoader): DataLoader for noisy test data.\n",
    "            test_clean_loader (DataLoader): DataLoader for clean test data.\n",
    "            num_epochs (int): The total number of epochs to train for.\n",
    "\n",
    "        Returns:\n",
    "            None. Prints training progress.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Generates and displays a plot of training and test loss curves over epochs.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def visualize_denoising(self, noisy_loader, clean_loader, num_images=5):\n",
    "        \"\"\"\n",
    "        Visualizes the denoising results by comparing original, noisy,\n",
    "        and reconstructed images side-by-side.\n",
    "\n",
    "        Args:\n",
    "            noisy_loader (DataLoader): DataLoader for noisy test images.\n",
    "            clean_loader (DataLoader): DataLoader for clean test images.\n",
    "            num_images (int, optional): The number of image triplets to display.\n",
    "                                        Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            None. Displays the comparison plot.\n",
    "        \"\"\"\n",
    "        # Use the dedicated plotting function plot_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e8e9f",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\">Model Training & Evaluation | <span style=\"color: #007BFF;\">Linear DAE</span></h2>\n",
    "\n",
    "- Instantiate the `LinearDenoisingAutoencoder` model.  \n",
    "- Create a `LinearDAETrainer` object to manage training.  \n",
    "- Start the training process\n",
    "- After training:\n",
    "  - Plot the loss curves to visualize the training progress.\n",
    "  - Display qualitative results:\n",
    "    - Show denoised images compared them with their corresponding noisy inputs and clean originals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d537bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Setup Linear DAE ==================\n",
    "\n",
    "\n",
    "# ===================== Train Model =====================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Plot Loss =====================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Visualize Results ================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c331c6d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1 style=\"text-align: left;\"><b>Task 2: Convolutional Denoising Autoencoder | <span style=\"color: #007BFF;\">MNIST</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "<p>Next, we implement a more sophisticated Denoising Autoencoder using <strong>Convolutional layers</strong> (`Conv2d`) in the encoder and <strong>Transposed Convolutional layers</strong> (`ConvTranspose2d`) in the decoder. This architecture is inherently better suited for image data as it preserves and utilizes the spatial structure (pixel neighborhoods) through learnable filters.\n",
    "\n",
    "\n",
    "<strong>üö® Important: When implementing the model, ensure that both the encoder and the decoder sections utilize maximum 3 relevant convolutional (`nn.Conv2d`) or transposed convolutional (`nn.ConvTranspose2d`) layers each. üö®</strong></p>\n",
    "\n",
    "\n",
    "\n",
    "<p>The encoder will progressively reduce the spatial dimensions (height, width) while increasing the number of channels (features), and the decoder will reverse this process to reconstruct the clean image.</p>\n",
    "\n",
    "<p>We explicitly separate the `encoder` and `decoder` components within the model definition. This allows us to easily extract the output of the encoder (the latent space representation) later for Task 3 (t-SNE visualization).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79521a99",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ConvDenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        \"\"\"\n",
    "        Initializes the layers of the Convolutional Denoising Autoencoder.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int, optional): The desired dimensionality of the latent space\n",
    "                                       (bottleneck). Defaults to LATENT_DIM hyperparameter.\n",
    "        \"\"\"\n",
    "        super(ConvDenoisingAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.encoder = ...\n",
    "        self.decoder = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the convolutional autoencoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing a batch of images.\n",
    "                              Expected shape: (batch_size, 1, 28, 28).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "                - latent (torch.Tensor): The latent space representation. Shape: (batch_size, latent_dim).\n",
    "                - reconstruction (torch.Tensor): The reconstructed image. Shape: (batch_size, 1, 28, 28).\n",
    "        \"\"\"\n",
    " \n",
    "        latent = ...\n",
    "        reconstruction = ...\n",
    "        return latent, reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6c231",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Trainer Class | <span style=\"color: #007BFF;\">Conv DAE</span></h2>\n",
    "<p>The `ConvDAETrainer` class mirrors the structure of the `LinearDAETrainer` but is adapted for the `ConvDenoisingAutoencoder`. Key differences include:</p>\n",
    "<ul>\n",
    "    <li>Handling image-shaped inputs and outputs directly (no flattening needed for loss calculation).</li>\n",
    "    <li>The `model.forward()` call returns both the latent vector and the reconstruction; only the reconstruction is used for calculating the denoising loss.</li>\n",
    "    <li>Includes a crucial `get_latent_features` method. This method takes a DataLoader (typically containing *clean* images), passes them through the *encoder part* of the trained model, and collects the resulting latent vectors along with their true labels. This data is essential for the t-SNE visualization in Task 3.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085746f0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ConvDAETrainer:\n",
    "    \"\"\"\n",
    "    Manages the training, evaluation, visualization, and latent feature extraction\n",
    "    for the ConvDenoisingAutoencoder model.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, learning_rate=0.001, device=None):\n",
    "        \"\"\"\n",
    "        Initializes the Trainer.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The ConvDenoisingAutoencoder model instance.\n",
    "            learning_rate (float, optional): Learning rate for the Adam optimizer. Defaults to 0.001.\n",
    "            device (torch.device, optional): Device ('cuda', 'mps', 'cpu') for training.\n",
    "                                             Auto-detects if None. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        # Display the model summary\n",
    "        summary(self.model, input_size=(1, 28, 28), device=str(self.device))\n",
    "\n",
    "\n",
    "\n",
    "    def train_epoch(self, noisy_loader, clean_loader):\n",
    "        \"\"\"\n",
    "        Performs a single epoch of training for the Convolutional DAE.\n",
    "\n",
    "        Args:\n",
    "            noisy_loader (DataLoader): DataLoader for noisy input images.\n",
    "            clean_loader (DataLoader): DataLoader for corresponding clean target images.\n",
    "\n",
    "        Returns:\n",
    "            float: Average training loss for the epoch.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "\n",
    "    def evaluate(self, noisy_loader, clean_loader):\n",
    "        \"\"\"\n",
    "        Evaluates the Convolutional DAE on the test dataset.\n",
    "\n",
    "        Args:\n",
    "            noisy_loader (DataLoader): DataLoader for noisy test images (input).\n",
    "            clean_loader (DataLoader): DataLoader for clean test images (target).\n",
    "\n",
    "        Returns:\n",
    "            float: Average test loss for the epoch.\n",
    "        \"\"\"\n",
    "        return 0\n",
    "    \n",
    "    def train(self, train_noisy_loader, train_clean_loader, test_noisy_loader, test_clean_loader, num_epochs):\n",
    "        \"\"\"\n",
    "        Runs the full training and evaluation loop for the Conv DAE.\n",
    "\n",
    "        Args:\n",
    "            train_noisy_loader (DataLoader): DataLoader for noisy training data.\n",
    "            train_clean_loader (DataLoader): DataLoader for clean training data.\n",
    "            test_noisy_loader (DataLoader): DataLoader for noisy test data.\n",
    "            test_clean_loader (DataLoader): DataLoader for clean test data.\n",
    "            num_epochs (int): Total number of epochs to train.\n",
    "\n",
    "        Returns:\n",
    "            None. Prints training progress.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def plot_loss(self):\n",
    "        \"\"\"\n",
    "        Generates and displays a plot of training and test loss curves over epochs.\n",
    "        \"\"\"\n",
    "      \n",
    "\n",
    "    def visualize_denoising(self, noisy_loader, clean_loader, num_images=5):\n",
    "        \"\"\"\n",
    "        Visualizes the denoising performance by comparing original, noisy,\n",
    "        and reconstructed images.\n",
    "\n",
    "        Args:\n",
    "            noisy_loader (DataLoader): DataLoader for noisy test images.\n",
    "            clean_loader (DataLoader): DataLoader for clean test images.\n",
    "            num_images (int, optional): Number of image triplets to display. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            None. Displays the comparison plot.\n",
    "        \"\"\"\n",
    "        # Use the comparison plot_comparison function\n",
    "\n",
    "\n",
    "    def get_latent_features(self, dataloader):\n",
    "        \"\"\"\n",
    "        Extracts the latent space representations for all items in a given dataloader.\n",
    "        This is typically used with the *clean* dataset after training to get latent\n",
    "        vectors corresponding to the original digits for visualization (e.g., t-SNE).\n",
    "\n",
    "        Args:\n",
    "            dataloader (DataLoader): The DataLoader containing the data (usually clean images)\n",
    "                                     for which to extract latent features.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.ndarray, np.ndarray]: A tuple containing:\n",
    "                - latent_features (np.ndarray): A NumPy array of the extracted latent vectors.\n",
    "                                                Shape: (num_samples, latent_dim).\n",
    "                - all_labels (np.ndarray): A NumPy array of the corresponding true labels.\n",
    "                                           Shape: (num_samples,).\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7be40",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Model Training & Evaluation | <span style=\"color: #007BFF;\">Conv DAE</span></h2>\n",
    "<p>Similar to Task 1, we now instantiate the `ConvDenoisingAutoencoder` and its `ConvDAETrainer`. We execute the training loop, plot the resulting loss curves, and visualize the denoising performance on sample test images. We expect the convolutional model to generally achieve lower reconstruction error and produce visually clearer reconstructions compared to the linear model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Setup Conv DAE ==================\n",
    "\n",
    "# ===================== Train Model =====================\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba477590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Plot Loss ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Visualize Results ================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487294ba",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1 style=\"text-align: left;\"><b>Task 3: t-SNE Visualization of Latent Space | <span style=\"color: #007BFF;\">Conv DAE</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "\n",
    "### <h2><b> Diving Deeper: Visualizing What the Autoencoder Learned </b></h2>\n",
    "<p>After training the Convolutional Denoising Autoencoder (which usually learns richer representations than the linear one), we want to understand the structure of the information captured in its compressed latent space. The encoder maps each input image (784 dimensions or 1x28x28) to a point in a lower-dimensional space (e.g., 64 dimensions defined by `LATENT_DIM`). While we can't directly \"see\" in 64 dimensions, we can use dimensionality reduction techniques to project these points into a 2D or 3D space that we *can* visualize.</p>\n",
    "\n",
    "### <h2><b> Introducing t-SNE: A Tool for High-Dimensional Data Visualization </b></h2>\n",
    "<p><strong>t-Distributed Stochastic Neighbor Embedding (t-SNE)</strong> is a state-of-the-art technique specifically designed for visualizing high-dimensional datasets in low dimensions (typically 2D or 3D). It's not a clustering algorithm itself, but it's excellent at revealing underlying cluster structures present in the data.</p>\n",
    "\n",
    "<h4><b>Core Idea: Preserving Neighborhoods</b></h4>\n",
    "<p>t-SNE works by modeling the similarity between high-dimensional data points and then trying to find a low-dimensional embedding where similar points remain close together and dissimilar points are pushed apart. It focuses on preserving the *local* structure of the data.</p>\n",
    "\n",
    "1.  **Modeling High-Dimensional Similarities:**\n",
    "    For every pair of high-dimensional points (our latent vectors), t-SNE computes a conditional probability\n",
    "    $p_{j|i}$ that point $i$ would pick point $j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $i$.\n",
    "    It then creates a joint probability:\n",
    "    $$p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n}$$\n",
    "    representing the similarity between $i$ and $j$.\n",
    "\n",
    "2.  **Modeling Low-Dimensional Similarities:**\n",
    "    It defines a similar joint probability $q_{ij}$ for the corresponding points in the low-dimensional map (e.g., 2D).\n",
    "    Crucially, it uses a heavy-tailed Student's t-distribution (with one degree of freedom) instead of a Gaussian here.\n",
    "    This allows moderately dissimilar points in the high-dimensional space to be modeled further apart in the low-dimensional map, helping to prevent crowding and better separating clusters.\n",
    "\n",
    "3.  **Minimizing Divergence:**\n",
    "    t-SNE uses gradient descent to adjust the positions of the points in the low-dimensional map to minimize the Kullback-Leibler (KL) divergence between the two distributions of joint probabilities ($P$ in high-D and $Q$ in low-D).\n",
    "    This optimization process effectively tries to make the low-dimensional map reflect the neighborhood structure of the high-dimensional data.\n",
    "\n",
    "DW we wont be doing this manually and will use the library!    \n",
    "\n",
    "<h4><b>Key Parameters (in `sklearn.manifold.TSNE`):</b></h4>\n",
    "<ul>\n",
    "    <li><b>`n_components` (int, default=2):</b> The target dimensionality (usually 2 for plotting).</li>\n",
    "    <li><b>`perplexity` (float):</b> This parameter loosely relates to the number of nearest neighbors considered for each point. It influences the balance between local and global aspects. Values between 5 and 50 are common. <strong>Tuning perplexity can significantly change the visualization.</strong> Lower values focus more on very local structure, while higher values consider broader neighborhoods.</li>\n",
    "    <li><b>`learning_rate` (float or 'auto', default='auto'):</b> Controls the step size during optimization. The default 'auto' setting (usually around 200) often works well. If the plot looks like a tight ball, the learning rate might be too low; if points seem randomly scattered, it might be too high.</li>\n",
    "    <li><b>`n_iter` (int):</b> The number of optimization iterations. t-SNE requires enough iterations to converge. 250 is the minimum recommended, 1000 or more is typical.</li>\n",
    "    <li><b>`init` (str or np.ndarray, default='random'):</b> Initialization method for the low-dimensional points. Using `'pca'` (`init='pca'`) often provides a more stable and globally consistent starting point than random initialization, potentially leading to better results and faster convergence.</li>\n",
    "    <li><b>`random_state` (int or None, default=None):</b> Seed for the random number generator. Setting this ensures reproducibility of the t-SNE plot.</li>\n",
    "    <li><b>`verbose` (int, default=0):</b> If > 0, prints progress messages during optimization. Useful for long runs.</li>\n",
    "</ul>\n",
    "\n",
    "<h4><b>How to Interpret the Plot:</b></h4>\n",
    "<ul>\n",
    "    <li><b>Clusters:</b> Look for distinct groups of points. In our case, we will color the points based on the true digit label (0-9). If the autoencoder learned meaningful features, we expect points corresponding to the same digit to form tight, relatively well-separated clusters.</li>\n",
    "    <li><b>Cluster Separation:</b> The visual separation between clusters suggests how distinct the learned representations are for different classes.</li>\n",
    "    <li><b>Caveats:</b>\n",
    "        <ul>\n",
    "            <li><strong>Distances between clusters are not always meaningful:</strong> t-SNE might place clusters far apart even if they were relatively close in the original space, or vice-versa. Focus on the grouping *within* clusters and the relative separation.</li>\n",
    "            <li><strong>Cluster sizes are not meaningful:</strong> The area occupied by a cluster in the t-SNE plot doesn't directly reflect the variance or number of points in the original space.</li>\n",
    "            <li>t-SNE is computationally intensive, especially for large datasets.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "<p>In this task, we will apply t-SNE to the latent vectors extracted from the <strong>Convolutional Denoising Autoencoder</strong> (using the clean test images as input to the encoder) and visualize the resulting 2D embedding.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48a668",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Extracting Latent Features | <span style=\"color: #007BFF;\">Conv DAE</span></h2>\n",
    "<p>First, we need the latent space representations. We use the `get_latent_features` method from our `ConvDAETrainer`. It's important to pass the <strong>clean test set DataLoader</strong> (`testloader_clean`) to this method. Why? Because we want to visualize how the autoencoder represents the <i>original, underlying structure</i> of the digits in its latent space, not how it represents the noisy inputs.</p>\n",
    "\n",
    "<p>Since t-SNE can be slow on large datasets (like the full 10,000 MNIST test images), you could use a random subset of 5000 data points for faster visualization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaafeea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292e2837",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Running t-SNE | <span style=\"color: #007BFF;\">Conv DAE Latent Space</span></h2>\n",
    "<p>Now we instantiate the `TSNE` object from `sklearn.manifold` with our chosen parameters (`n_components=2`, `perplexity`, `n_iter`, `init='pca'`, `random_state` for reproducibility, `verbose=1` to see progress). We then call the `fit_transform` method, passing in the extracted latent features. This performs the t-SNE optimization and returns the 2D coordinates for each latent vector.</p>\n",
    "<p><i>Note: This step can take several minutes depending on the number of samples and the dimensionality of the latent space.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d106f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ===================== Run t-SNE Algorithm ======================\n",
    "# Initialize the t-SNE transformer object\n",
    "\n",
    "\n",
    "# Apply t-SNE to the (subset of) latent features.\n",
    "# ==========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3795afd",
   "metadata": {},
   "source": [
    "### <h2 style=\"text-align: left;\"> Plotting t-SNE Results | <span style=\"color: #007BFF;\">Conv DAE Latent Space</span></h2>\n",
    "<p>Finally, we create a scatter plot of the 2D points generated by t-SNE. Each point represents an image from our test set (or subset). We use the `true_labels` (the actual digit, 0-9) to color-code the points. This allows us to visually inspect whether the latent space learned by the Convolutional DAE has successfully grouped images of the same digit together.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5367110",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10)) # Create a reasonably large figure\n",
    "# Create the scatter plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fabca",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## <h1><b> Theory Questions | <span style=\"color: #007BFF;\">Assignment 5</span></b></h1>\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Instructions:** Answer the following questions based on the concepts and implementations covered in this assignment. PLEASE BE BRIEF. No gpt taqreer. Actually yk what negative marking if you violate this rule\n",
    "\n",
    "1.  **Core Autoencoder Concepts:**\n",
    "    * a) What is the \"bottleneck\" or \"latent space\" in an autoencoder, and why is it important for learning useful representations?\n",
    "\n",
    "    **Answer:**\n",
    "\n",
    "2.  **Architectural Choices:**\n",
    "    * a) What is the role of the `ConvTranspose2d` layers (Transposed Convolution) in the decoder of the Convolutional DAE?\n",
    "\n",
    "     **Answer:**\n",
    "     \n",
    "\n",
    "3.  **Comparison and Analysis:**\n",
    "    * a) Based on the visual results and the final test loss values, which model (Linear DAE or Convolutional DAE) performed better at the denoising task? Provide a reason for this difference(if there is any).\n",
    "\n",
    "     **Answer:**\n",
    "\n",
    "\n",
    "    * b) Can the encoder part of a trained DAE be used for other downstream tasks? If so, how and why might it be useful?\n",
    "\n",
    "     **Answer:**\n",
    "\n",
    "    * c) Do you notice anything special about the clusters of digits '4' and '9' in the t-SNE plot? If so why might this be?\n",
    "\n",
    "    **Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1412ea",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "Best of luck with your assignment! If you have any questions or need further assistance, feel free to ask. Happy coding!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
