{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abe7c5a",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **PA4:  Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e305354",
   "metadata": {},
   "source": [
    "## Section 1: Introduction to PyTorch [10 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4356fb9",
   "metadata": {},
   "source": [
    "### Task 0: Installing PyTorch\n",
    "To install PyTorch on your local machine, please go through the official [guide](https://pytorch.org/get-started/locally/). Alternatively, you can use Google Colab for the assignment which has PyTorch installed and configured out of the box. Also, go through the official PyTorch [tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dedeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02bab49",
   "metadata": {},
   "source": [
    "### Task 1: Understanding Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28e812",
   "metadata": {},
   "source": [
    "Implement the following functions using PyTorch. You are not allowed to change any function definitions. You may find the following PyTorch utilities helpful. You may read up more in the [documentation](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "```python\n",
    "torch.tensor([...])         # Create tensor from list or nested list\n",
    "torch.arange(start, end)    # Range of values (1D)\n",
    "torch.ones(size)            # Tensor filled with ones\n",
    "torch.zeros(size)           # Tensor filled with zeros\n",
    "torch.rand(size)            # Uniformly random values in [0, 1)\n",
    "torch.randn(size)           # Normally distributed values\n",
    "\n",
    "tensor + other              # Element-wise addition\n",
    "tensor - other              # Element-wise subtraction\n",
    "tensor * other              # Element-wise multiplication\n",
    "tensor / other              # Element-wise division\n",
    "tensor @ other              # Matrix multiplication (preferred over matmul)\n",
    "\n",
    "tensor.shape                # Get the shape (tuple of dimensions)\n",
    "tensor.dtype                # Data type (e.g., float32, int64)\n",
    "tensor.device               # Shows whether on CPU or GPU\n",
    "\n",
    "tensor.reshape(new_shape)     # Change shape (returns a copy)\n",
    "tensor.view(new_shape)        # Alternative to reshape (may share memory)\n",
    "tensor.T                      # Transpose 2D tensor\n",
    "tensor.transpose(dim0, dim1)  # Swap two dimensions\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef4003",
   "metadata": {},
   "source": [
    "$$\n",
    "f_1 = a + \\mathbf{1}\n",
    "$$\n",
    "\n",
    "Where $a$ can be a tensor of any shape, and $1$ dynamically takes the shape of $a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892e391",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function Definition.\n",
    "def f1(a):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe54c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b1a02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "$$\n",
    "f_2 = (A \\cdot v)^T \\odot v^T\n",
    "$$\n",
    "\n",
    "- $A$ = $\\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9\n",
    "\\end{pmatrix}$\n",
    "- $v$ can be any vector of appropriate shape, your function should explicitly check this and return 0 if v is not of appropriate shape. \n",
    "- $\\odot$ represents element wise multiplication \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae23b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function Definition.\n",
    "def f2(v):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb609ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e504c9",
   "metadata": {},
   "source": [
    "### Task 2: Understanding Devices in PyTorch (CPU, CUDA, MPS)\n",
    "\n",
    "PyTorch can run tensor computations on different devices: `cpu` (default), `cuda` (for NVIDIA GPUs), and `mps` (for Apple Silicon GPUs). These devices speed up computation, especially for large tensors or deep learning models.\n",
    "\n",
    "To use a device, you must **explicitly send tensors to it** using `.to(device)` or create them directly on the device. If two tensors are involved in an operation, **they must be on the same device**. Otherwise, PyTorch will raise a runtime error.\n",
    "\n",
    "Below is an example setup — uncomment the line matching your hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b3b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # For NVIDIA GPU\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # For Apple Silicon\n",
    "\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bda507",
   "metadata": {},
   "source": [
    "Now implement the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872d346",
   "metadata": {},
   "source": [
    "$$\n",
    "f_3 = r^T \\cdot X\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $r \\in \\mathbb{R}^{4 \\times 1}$ is a random tensor generated by `torch.rand`\n",
    "- $X \\in \\mathbb{R}^{4 \\times 5}$ is a tensor with values from 1 to 20 (inclusive), reshaped to shape $(4, 5)$\n",
    "- Make sure the function explicitly sends r and X to whatever device is passed. The returned tensor should also be on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae22450",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function Definition.\n",
    "def f3(device):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60c1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test your function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb90a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda42b3",
   "metadata": {},
   "source": [
    "## Section 2: Feed Forward Neural Networks [30 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cf512",
   "metadata": {},
   "source": [
    "### Task 1: MNIST Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6656a",
   "metadata": {},
   "source": [
    "#### Step 1: Loading Data [10 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3840c2d1",
   "metadata": {},
   "source": [
    "In PyTorch, `Dataset` is an abstract class representing a dataset, it allows you to access individual raw data points. Firstly you will load the MNIST dataset using `torchvision.datasets.MNIST`. This dataset contains images of handwritten digits from 0 to 9, each of size 28x28 pixels. We will load the training data here. The dataset will automatically be downloaded if it is not already present on your local machine. You may refer to the documentation [here](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html?highlight=mnist#torchvision.datasets.MNIST). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84920743",
   "metadata": {},
   "source": [
    "Next, we will define a `transform` to apply to the dataset. The most basic transformation would be to convert the images to tensors. Another common practice is to normalize the pixel values so that the network can train more efficiently. Transformations are applied every time a sample is fetched from the dataset. You have to define a transform function that would convert images to tensors and normalizes them. You may explore further by referring to the documentation [here](https://pytorch.org/vision/0.9/transforms.html).\n",
    "\n",
    "Functions that will come in handy are:\n",
    "```python\n",
    "torchvision.transforms.Compose()\n",
    "torchvision.transforms.ToTensor()\n",
    "torchvision.transforms.Normalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a2301",
   "metadata": {},
   "source": [
    "Next we will create dataloaders. A `DataLoader` is an iterable pytorch object that automatically splits the dataset into mini-batches, shuffles the data, applies the transformations we defined earlier, and loads data in parallel for efficient training. You may look into dataloaders [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "\n",
    "We will also split the MNIST dataset into a training, validation and testing set. You may look into the `random_split` function to randomly divide the dataset. \n",
    "\n",
    "Your task is to:\n",
    "1. Split the dataset into training, validation and testing datasets.\n",
    "2. Create two `DataLoader` instances: one for the training data and one for the test data.\n",
    "3. Set a batch size of 64 and shuffle the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466487f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = ...\n",
    "val_loader = ...\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d1d96",
   "metadata": {},
   "source": [
    "Now, let's visualize the first batch of images from the training data. You may use `matplotlib` or PyTorch utilities such as `torch.utils.data.make_grid` to plot the images and display their corresponding labels. Always visualize because it helps us ensure that the data is correctly loaded and transformed.\n",
    "\n",
    "Task:\n",
    "- Display 8 images from the any batch of training data.\n",
    "- Show the corresponding labels for those images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))  # Get the first batch of data (remember train_loaders are ITERABLE, and every iteration contains 1 mini-batch of data)\n",
    "\n",
    "### Visualization Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a1561",
   "metadata": {},
   "source": [
    "#### Step 2: Creating a model [10 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1f8d7",
   "metadata": {},
   "source": [
    "In this step, we will define the architecture of a simple Feed-Forward Neural Network (FFNN) for MNIST classification. The network will have:\n",
    "- An input layer corresponding to the size of the MNIST images. (Think about how to tackle dimensionality here)\n",
    "- A few fully connected hidden layers with simple activation functions (e.g., ReLU).\n",
    "- An output layer (Again think about the dimensionality here)\n",
    "- Make sure that your total parameters do not exceed 600,000\n",
    "\n",
    "\n",
    "We will use `torch.nn.Module` to define the model structure. You are free to choose any appropriate activation functions of your choice. You can look into the documentation [here](https://pytorch.org/docs/stable/nn.html), some useful functions that you should look up are:\n",
    "```python\n",
    "nn.Linear()\n",
    "nn.Sequential()\n",
    "nn.Sigmoid()\n",
    "nn.ReLU()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFFNN, self).__init__()\n",
    "        \n",
    "        # You are free to change the architecture of the network.\n",
    "        self.input_layer = ...\n",
    "        self.hidden_layers = ...\n",
    "        self.output_layer = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02443c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = SimpleFFNN()\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# Print the model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c8c22",
   "metadata": {},
   "source": [
    "#### Step 3: Training your model [10 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b98e1",
   "metadata": {},
   "source": [
    "Before we begin training, we will need to set up a loss function and an optimizer for training our model.\n",
    "\n",
    "In this assignment, we will use `nn.CrossEntropyLoss` for classification. Like any loss function, CELoss would expect `y_true` and `y_predicted`. One thing I want you to think about is whether you should pass raw logits or softmax outputs as y_predicted, and why. (This may also have an implication on your model's forward pass). Make sure you look at the example in the documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "The `optimizer` is a PyTorch class that updates the model's weights based on the gradients computed during backpropagation. For this assignment our optimizer will use simple Stochastic Gradient Descent, `SGD`, that you studied in class. You may look at the documentation [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). \n",
    "\n",
    "\n",
    "Useful functions that you should know are:\n",
    "```python\n",
    "loss.backward() # Does backpropagation after calculating the loss\n",
    "optimizer.zero_grad() # clears old gradients before computing new ones to prevent accumulation\n",
    "optimizer.step() # updates the weights based on the computed gradients.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = ...\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90079486",
   "metadata": {},
   "source": [
    "We can finally begin training our network. You should be careful when handling gradients, loss computation, and weight updates through backpropagation.\n",
    "\n",
    "Since we might have to train our model multiple times, it is good practice to write a training loop. The training loop should be a function that does the following:\n",
    "\n",
    "1. Forward Pass: Pass the input data through the model to get predictions (logits).\n",
    "2. Loss Calculation: Compute the loss by comparing the logits and the true labels.\n",
    "3. Zero Gradients: Call `optimizer.zero_grad()` to reset the gradients from the previous iteration.\n",
    "4. Backward Pass: Call `loss.backward()` to compute the gradients of the loss with respect to each model parameter. This step performs backpropagation.\n",
    "5. Update Weights: Call `optimizer.step()` to update the model's weights based on the computed gradients.\n",
    "\n",
    "Note that\n",
    "- Backpropagation: `loss.backward()` computes the gradient of the loss for each parameter in the model. These gradients are used by the optimizer to update the model parameters.\n",
    "- Ensure that gradients are reset at the start of each iteration using `optimizer.zero_grad()`, so they don't accumulate.\n",
    "- Make sure you move both your model and inputs to device.\n",
    "- It may be a good idea to print the loss after every few iterations, perhaps you may also look into `tqdm` [here](https://tqdm.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, train_loader, test_loader, optimizer, loss_function, device):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c8cf0",
   "metadata": {},
   "source": [
    "It is also a useful to have a function to evaluate the model's performance on test data. We will use an `evaluate` function that takes the model and dataloader. Since we do not need any backpropagation during inference, you should wrap your function in `torch.no_grad()` to disable gradient computation, saving memory and computation. Your function should return both average accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caca3f5",
   "metadata": {},
   "source": [
    "Understanding Training Terminology:\n",
    "\n",
    "`Epoch`: One complete pass through the entire training dataset. Training over multiple epochs allows the model to learn progressively.\n",
    "\n",
    "`Batch`: A small subset of the dataset used to compute the gradient and update the model. Mini-batches enable efficient computation and provide more frequent updates compared to processing the full dataset at once.\n",
    "\n",
    "`Iteration`: One update of the model's parameters, which occurs after processing a single mini-batch. The total number of iterations per epoch is the number of training samples divided by the batch size.\n",
    "\n",
    "You should now use your train loop to train your model upto a desired number of epochs. It may be a good idea to use your `val_loader` to evaluate the model after every epoch. This will allow you to see if the model generalizes well and potentially detect any overfitting. Make sure that you record your average train and validation loss for each epoch because you will have to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1446aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = ...\n",
    "# Train the model for a desired number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3492044",
   "metadata": {},
   "source": [
    "Now you should plot your loss curves (for both train and validation) for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9460f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0312307a",
   "metadata": {},
   "source": [
    "Finally, we will use `test_loader` to evaluate our trained model. Remember that the accuracy here should be **atleast 95%**. If the desired accuracy is not reached, you should first check your implementation, and if its correct, then start tweaking the hyperparameters and retrain your model until you reach the desired accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b823449",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be193b17",
   "metadata": {},
   "source": [
    "#### Step 4: Saving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db6ea1",
   "metadata": {},
   "source": [
    "In PyTorch, there are two main ways to save models: **state dictionary** and **full model saving**.\n",
    "\n",
    "State Dictionary:\n",
    "- The `state_dict` contains all the parameters of the model (weights and biases).\n",
    "- Saving the state_dict allows you to only store the learned parameters, making the saved file smaller and more flexible. You can load it into any model architecture that matches the saved parameters.\n",
    "\n",
    "Full Model Saving:\n",
    "- Saving the full model stores both the model's architecture and its parameters. This method is more convenient as it preserves the entire model structure but results in larger file sizes.\n",
    "\n",
    "Why Save Your Model?\n",
    "Saving your model is crucial because once the kernel crashes or you close your laptop, you lose your model and its training progress. Saving the model ensures you can resume from where you left off, perform inference, or share your trained model with others.\n",
    "\n",
    "Once you have saved your model, you can load it back into memory using `torch.load()`. This allows you to restore the model's architecture and learned parameters, enabling you to run inference or continue training from where you left off.\n",
    "\n",
    "For this assignment, we will save the full model, as it will be used for inference and grading. Use `torch.save()` to save the model. This will allow us to run inference on the saved model during evaluation and grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4080072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, 'xyz.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4994fca",
   "metadata": {},
   "source": [
    "## Section 3: Convolutional Neural Networks [30 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e99483e",
   "metadata": {},
   "source": [
    "### Step 1: Loading & Visualizing CIFAR-10\n",
    "\n",
    "You now understand the flow of training a model. Let's now train a Convolutional Neural Network using our knowledge from Part 2. Like always, the first step is to load CIFAR-10 and Construct Dataloaders.\n",
    "\n",
    "- Use torchvision.datasets.CIFAR10 to download the dataset.\n",
    "- Apply transformations: convert images to tensors and normalize them using mean [0.4914, 0.4822, 0.4465] and std [0.2470, 0.2435, 0.2616] for each RGB channel.\n",
    "- Split the dataset into training (80%), validation (10%), and test (10%) sets.\n",
    "- Create DataLoader instances with a reasonable batch size (32 or 64), make sure you shuffle the training data.\n",
    "- Visualize atleast 10 samples from the training set including their labels (label and not label index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52249be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ...  # Define transforms (ToTensor + Normalize)\n",
    "\n",
    "dataset = ...    # Download CIFAR-10 with transforms\n",
    "\n",
    "\n",
    "train_loader = ...  # DataLoader for training\n",
    "val_loader = ...    # DataLoader for validation\n",
    "test_loader = ...   # DataLoader for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e605dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize 10 Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b160ee",
   "metadata": {},
   "source": [
    "### Step 2: Model Architecture\n",
    "\n",
    "Design a Simple CNN model. Ensure the model handles 3-channel input (RGB) and outputs 10 classes. Make sure that total parameters must be less than `5 million`.\n",
    " \n",
    "You may find the following modules useful:\n",
    "\n",
    "```python\n",
    "nn.Conv2d \n",
    "nn.MaxPool2d \n",
    "nn.Dropout \n",
    "nn.Linear \n",
    "nn.ReLU\n",
    "nn.Sigmoid\n",
    "nn.BatchNorm2d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e530c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c560b3c",
   "metadata": {},
   "source": [
    "Instantiate your model and print the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d43dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ...\n",
    "num_params = ...\n",
    "\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429b5c7",
   "metadata": {},
   "source": [
    "### Step 3: Train your model\n",
    "\n",
    "Reuse the `train_loop` and `evaluate` functions from Part 2. Declare an appropriate loss function and optimizer. Adjust hyperparameters (e.g., learning rate) as needed. You should train your model for no more than `10 epochs`. Make sure you plot your loss curves (train and validation) for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513b545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "183979de",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate your model\n",
    "\n",
    "You should evaluate your model on the test dataloader now. Make sure that your test accuracy is atleast `70%`. If it is less then unfortunately you will have to tweak your hyperparameters or model architectures and retrain (or lose some marks?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f9007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcdc18c0",
   "metadata": {},
   "source": [
    "### Step 5: Save your model\n",
    "\n",
    "\n",
    "Save your final model as a `.pth` file. Make sure you save the entire model and not just the state dictionary. We will load your model and test it, so make sure you save your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afe6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "845ed2e9",
   "metadata": {},
   "source": [
    "## Section 4: Exploration [30 Marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e3964",
   "metadata": {},
   "source": [
    "You are to attempt any 2 tasks out of the 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ffbfc",
   "metadata": {},
   "source": [
    "### Task 1: Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0b20c",
   "metadata": {},
   "source": [
    "Transfer learning is the process of adapting a pre-trained model to a new, related task by fine-tuning its parameters, thereby leveraging the general features learned from large datasets such as ImageNet. Pretrained models, also known as foundation models, capture a diverse range of features that are useful for many vision tasks, and fine-tuning these models requires less computational resources and data compared to training a network from scratch.\n",
    "\n",
    "This task will help you understand the process of working with pretrained models in PyTorch, focusing on importing the VGG11 model pretrained on ImageNet, freezing its backbone, and fine-tuning a new classifier head for CIFAR10. \n",
    "\n",
    "Follow the steps below to understand the workflow.\n",
    "\n",
    "You should do the following steps:\n",
    "- You should import a pretrained [VGG11]((https://pytorch.org/vision/main/models/generated/torchvision.models.vgg11.html).) model from torchvision.\n",
    "- By importing `VGG11_Weights` and then using the `ImageNet22k_Weights.DEFAULT` option within the model loading function, the VGG11 model will be initialized with weights obtained from extensive training on approximately 14 million images across 22,000 classes.\n",
    "- To adapt the pretrained VGG11 model for CIFAR10, which has 10 classes, a common strategy is to freeze the backbone and train a new classifier head. \n",
    "- The Backbone refers to the convolutional base of the VGG11 model responsible for feature extraction.\n",
    "- Freezing involves setting the parameters of the backbone layers so that their values do not change during training (for example, by setting `requires_grad=False` on these parameters).\n",
    "- This approach leverages the already learned features from ImageNet, reducing the training time and required amount of CIFAR10 data.\n",
    "- After freezing the backbone, the final classification layer (or head) is replaced or modified to suit the CIFAR10 classification task.\n",
    "- The new classifier head is initialized with random weights and is trained on CIFAR10 data.\n",
    "- Only the parameters of this new head are updated during training, which helps in adapting the model’s predictions to the specifics of CIFAR10.\n",
    "- Finally, you should evaluate the accuracy of your pretrained model on CIFAR-10 and compare it with when you trained the model from scratch in the previous part. Also comment briefly on the training time/compute required for both of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You are free to make as many cells as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2b36d",
   "metadata": {},
   "source": [
    "### Task 2: GradCAM Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8154835",
   "metadata": {},
   "source": [
    "GradCAM (Gradient-weighted Class Activation Mapping) provides visual explanations for CNN decisions by highlighting important regions in an image. This guide walks through setting up GradCAM with a pretrained model, visualizing feature maps, and additional tips for interpretability. \n",
    "\n",
    "GradCAM reveals where a CNN looks to decide on a class. By computing the gradient of the output with respect to a convolutional layer's features and applying a ReLU on the weighted sum, a heatmap is produced. Overlaying this heatmap on the original image shows the regions influencing the model's decision. You need to refer to the [official repository](https://github.com/jacobgil/pytorch-grad-cam) here to see usage examples. \n",
    "\n",
    "To conduct a gradcam analysis, you need the following:\n",
    "- Your CNN Model from Part 3\n",
    "- Select a target layer. \n",
    "- Select a target class. For example, `ClassifierOutputTarget(0)` would visualize for class 0.\n",
    "- Generate and Visualize the Heatmap.\n",
    "\n",
    "Your task is to preprocess 6 input images from different classes, generate the GradCAM heatmap, and overlay it on the original image. You are to submit these visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbe138",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You are free to make as many cells as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccf168",
   "metadata": {},
   "source": [
    "### Task 3: Out of Distribution (OOD) Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d0ad8",
   "metadata": {},
   "source": [
    "Machine learning models often perform remarkably well on test data drawn from the same distribution as their training data. However, they can fail drastically when faced with Out-of-Distribution (OOD) inputs—data that deviates from what the model has seen during training.\n",
    "\n",
    "Imagine a model trained to classify images of animals. It may correctly identify a photo of a horse but struggle with a sketch of a horse, even though both are semantically the same. This is a classic case of OOD generalization failure. Similarly, a model trained only on images taken during the day may perform poorly on images captured at night, even though the underlying objects remain the same.\n",
    "\n",
    "In this task, you will investigate how robust your models are when faced with such distributional shifts by conducting a simple experiment. Your MNIST classifier has likely never seen digits written by you. In this task, you will generate OOD data by manually writing digits on paper, photographing them, and testing the model’s predictions.\n",
    "\n",
    "Steps:\n",
    "- Write digits 0–9 on a blank sheet of paper.\n",
    "- Take clear, well-lit photos of the sheet.\n",
    "- Load and Visualize these JPEGs\n",
    "- We now preprocess each image. Convert it to grayscale tensors.\n",
    "- Invert the colors (so the digits are white on a black background).\n",
    "- Resize it to 28x28 pixels using transforms.Resize\n",
    "- Perform any other transforms that you did on your original MNIST dataset such as normalization.\n",
    "- Pass these images through your trained MNIST model and observe how it performs.\n",
    "- Comment on your accuracy and on which digits were easier or harder to classify and hypothesize why.\n",
    "\n",
    "By conducting these experiments, you'll gain a better understanding of how fragile deep learning models can be when moved away from their original training distribution—and why OOD robustness is an important topic in real-world deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You are free to make as many cells as you want"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
